{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Transformer model for language understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swymtxpl7W7w"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLIcf0VlEpbx",
        "outputId": "beb93269-8198-4dc4-e510-b1fdf79fd39b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_datasets\n",
            "  Using cached tensorflow_datasets-4.5.2-py3-none-any.whl (4.2 MB)\n",
            "Requirement already satisfied: absl-py in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from tensorflow_datasets) (1.0.0)\n",
            "Requirement already satisfied: termcolor in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from tensorflow_datasets) (1.1.0)\n",
            "Collecting dill\n",
            "  Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
            "Requirement already satisfied: typing-extensions in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from tensorflow_datasets) (4.1.1)\n",
            "Requirement already satisfied: six in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from tensorflow_datasets) (1.16.0)\n",
            "Collecting tqdm\n",
            "  Using cached tqdm-4.63.0-py2.py3-none-any.whl (76 kB)\n",
            "Collecting promise\n",
            "  Using cached promise-2.3.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting requests>=2.19.0\n",
            "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "Requirement already satisfied: numpy in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from tensorflow_datasets) (1.18.5)\n",
            "Requirement already satisfied: importlib-resources in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from tensorflow_datasets) (5.4.0)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from tensorflow_datasets) (3.19.4)\n",
            "Collecting tensorflow-metadata\n",
            "  Using cached tensorflow_metadata-1.7.0-py3-none-any.whl (48 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
            "Collecting charset-normalizer~=2.0.0\n",
            "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from importlib-resources->tensorflow_datasets) (3.7.0)\n",
            "Collecting googleapis-common-protos<2,>=1.52.0\n",
            "  Using cached googleapis_common_protos-1.55.0-py2.py3-none-any.whl (212 kB)\n",
            "Building wheels for collected packages: promise\n",
            "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=cdd3e1d89ac417fe5dd4a603274aa13f2efe9ef067bf3d6928416b910b98d1b0\n",
            "  Stored in directory: /home/nathan/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
            "Successfully built promise\n",
            "Installing collected packages: certifi, urllib3, tqdm, promise, idna, googleapis-common-protos, dill, charset-normalizer, tensorflow-metadata, requests, tensorflow_datasets\n",
            "Successfully installed certifi-2021.10.8 charset-normalizer-2.0.12 dill-0.3.4 googleapis-common-protos-1.55.0 idna-3.3 promise-2.3 requests-2.27.1 tensorflow-metadata-1.7.0 tensorflow_datasets-4.5.2 tqdm-4.63.0 urllib3-1.26.9\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from transformers) (4.11.3)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hRequirement already satisfied: requests in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from transformers) (4.63.0)\n",
            "Collecting filelock\n",
            "  Using cached filelock-3.6.0-py3-none-any.whl (10.0 kB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.2/895.2 KB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting regex!=2019.12.17\n",
            "  Downloading regex-2022.3.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m749.6/749.6 KB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from transformers) (1.18.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 KB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
            "Collecting joblib\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Requirement already satisfied: six in /home/nathan/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
            "Collecting click\n",
            "  Using cached click-8.0.4-py3-none-any.whl (97 kB)\n",
            "Installing collected packages: tokenizers, regex, pyyaml, joblib, filelock, huggingface-hub, click, sacremoses, transformers\n",
            "Successfully installed click-8.0.4 filelock-3.6.0 huggingface-hub-0.4.0 joblib-1.1.0 pyyaml-6.0 regex-2022.3.15 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install tensorflow\n",
        "!pip install tensorflow_datasets\n",
        "!pip install transformers\n",
        "# !pip install -U \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-directml 1.15.5 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.21.5 which is incompatible.\n",
            "tensorflow-directml 1.15.5 requires tensorboard<1.16.0,>=1.15.0, but you have tensorboard 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow>2.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1BPAGqMMLbNP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from random import randint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ax44X7m8ZePx"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JjJJyJTZYebt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-03-17 11:50:36.358615: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "2022-03-17 11:50:36.368220: I tensorflow/stream_executor/platform/default/dso_loader.cc:97] Successfully opened dynamic library libdirectml.24bfac66e4ee42ec393a5fb471412d0177bc7bcf.so\n",
            "2022-03-17 11:50:36.368359: I tensorflow/stream_executor/platform/default/dso_loader.cc:97] Successfully opened dynamic library libdxcore.so\n",
            "2022-03-17 11:50:36.370361: I tensorflow/stream_executor/platform/default/dso_loader.cc:97] Successfully opened dynamic library libd3d12.so\n",
            "2022-03-17 11:50:36.737923: I tensorflow/core/common_runtime/dml/dml_device_cache.cc:250] DirectML device enumeration: found 2 compatible adapters.\n",
            "2022-03-17 11:50:36.738211: I tensorflow/core/common_runtime/dml/dml_device_cache.cc:186] DirectML: creating device on adapter 0 (NVIDIA GeForce RTX 3080 Laptop GPU\n",
            "2022-03-17 11:50:36.825142: I tensorflow/core/common_runtime/dml/dml_device_cache.cc:186] DirectML: creating device on adapter 1 (Intel(R) UHD Graphics\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/device:DML:0'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import collections\n",
        "import logging\n",
        "import os\n",
        "# import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "kVd9vcO-CQnT"
      },
      "outputs": [],
      "source": [
        "project_path = '.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pXzVhU34zWEU"
      },
      "outputs": [],
      "source": [
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cCvXbPkccV1"
      },
      "source": [
        "## Download the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT9A8c2Lcg5L",
        "outputId": "e93f6790-0f5c-49d1-ff52-04bf2abd7eba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using the https://tfhub.dev/google/wiki40b-lm-en/1 model to generate sequences of max length 20.\n"
          ]
        }
      ],
      "source": [
        "language = \"en\" #[\"en\", \"ar\", \"zh-cn\", \"zh-tw\", \"nl\", \"fr\", \"de\", \"it\", \"ja\", \"ko\", \"pl\", \"pt\", \"ru\", \"es\", \"th\", \"tr\", \"bg\", \"ca\", \"cs\", \"da\", \"el\", \"et\", \"fa\", \"fi\", \"he\", \"hi\", \"hr\", \"hu\", \"id\", \"lt\", \"lv\", \"ms\", \"no\", \"ro\", \"sk\", \"sl\", \"sr\", \"sv\", \"tl\", \"uk\", \"vi\", \"multilingual-64k\", \"multilingual-128k\"]\n",
        "hub_module = \"https://tfhub.dev/google/wiki40b-lm-{}/1\".format(language)\n",
        "max_gen_len = 20 \n",
        "# max gen len works how? not working yet\n",
        "print(\"Using the {} model to generate sequences of max length {}.\".format(hub_module, max_gen_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xom08TExfYXg",
        "outputId": "0f784ab3-82a4-458a-e643-470ab48cb8e6"
      },
      "outputs": [],
      "source": [
        "# print(list(tfds.list_builders()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8q9t4FmN96eN"
      },
      "outputs": [],
      "source": [
        "examples, metadata = tfds.load('wiki40b', with_info=True, try_gcs=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1F7MbEUGXIHI"
      },
      "outputs": [],
      "source": [
        "train_examples = train_examples.prefetch(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OnwQZ2IpTbl"
      },
      "source": [
        "The `tf.data.Dataset` object returned by TensorFlow datasets yields pairs of text examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtuMLTQGlHZ1",
        "outputId": "8eaa82d2-c9b0-45f7-ab86-f13f98b8c718"
      },
      "outputs": [],
      "source": [
        "# for en_examples in train_examples.batch(3).take(1):\n",
        "\n",
        "#   for en in en_examples:\n",
        "#     print(en, en_examples.get(en))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJxTd6aVnZyh"
      },
      "source": [
        "## Text tokenization & detokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlHAFNaopq6U"
      },
      "source": [
        "You can't train a model directly on text. The text needs to be converted to some numeric representation first. Typically, you convert the text to sequences of token IDs, which are used as indices into an embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sPvbCEePzOC"
      },
      "source": [
        "One popular implementation is demonstrated in the [Subword tokenizer tutorial](https://www.tensorflow.org/text/guide/subwords_tokenizer) builds subword tokenizers (`text.BertTokenizer`) optimized for this dataset and exports them in a [saved_model](https://www.tensorflow.org/guide/saved_model). \n",
        "\n",
        "Download and unzip and import the `saved_model`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "7f006303fc2c419fb09cf892b181a08c",
            "2a5eae85e5eb43d7abe9773a72015dad",
            "559d6d68e25f4a67b6397f23e85efd07",
            "d192779a902048528a22d8f8fcb0d54f",
            "17bac3e1c9d74664a57097206907f829",
            "c5bb3969275d451d8add76cbb935eeff",
            "43750abaf77c4954acbd000a8c00183c",
            "541b3afe67ab41838f51fbcac7f9f379",
            "75530da518a941e88e38e1581547f75b",
            "e4b96ef6a6f64ea8af72f4813a0e7ce5",
            "dc66da1970b949c2b2bec1f3a87405c3",
            "168d699b4f0047f1b2cf91506a734c40",
            "a16dc0b40e574e0596325474dc361c03",
            "1ab1e05b56834c1fa12b09ac2e5bb826",
            "3576090d28654cf2ba0ccb9ad885b72c",
            "f4b63b655b4a44d3b04d36e9d66239bb",
            "a9a68829fa634310895b25f25c3c2c32",
            "b7f2098fd20847db89f41b2b007e7e06",
            "6ff7c26f261a4842a355dab24f83c17c",
            "ac736c3420f243059a435030c8489458",
            "a7ea46cb41ea4ab0aa6a4eed19f60c6f",
            "584f2f021a7e4a4b9d20381b8963d6c1",
            "283c81df98ad474aac287455d2c0fc5e",
            "7639d95150314e78a5ea2ef00350f4e7",
            "0599716d3c0e435f9dc3130b42b14bea",
            "752babd396f14c0495666352d67ceb4c",
            "5478a53244b64995bc8d0d20bd97f65f",
            "499923c8538c4508ac0bc51efeefe85d",
            "4f2b0c7d78c1421d9be36528cc97c8eb",
            "49e6c8996c01473586127284db1f5969",
            "dc635c94f187432195a8c6bbf4908c02",
            "d32be5bb06cc480c85c2cd78805d91c3",
            "061ef147b481472fb09d67a310da5839"
          ]
        },
        "id": "iB7Ydx6S0vrW",
        "outputId": "802d0742-c0b0-4377-e80b-7ff377400905"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89761a61031a459ab5960edf14e672ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "319400ae5c684cd8bd25cf8cd6edcc87",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdd597d1d5a747f78627abf5a4ff0cbe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QToMl0NanZPr"
      },
      "outputs": [],
      "source": [
        "# model_name = \"ted_hrlr_translate_pt_en_converter\"\n",
        "# tf.keras.utils.get_file(\n",
        "#     f\"{model_name}.zip\",\n",
        "#     f\"https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip\",\n",
        "#     cache_dir='.', cache_subdir='', extract=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "h5dbGnPXnuI1"
      },
      "outputs": [],
      "source": [
        "# tokenizers = tf.saved_model.load(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-PCJijfcZ9_",
        "outputId": "080277b7-9023-43cd-d010-12c4a209bc62"
      },
      "outputs": [],
      "source": [
        "# [item for item in dir(tokenizer) if not item.startswith('_')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDeI0Dqn3E9V",
        "outputId": "78a397d0-a16f-48c6-c7b8-7359092931e0"
      },
      "outputs": [],
      "source": [
        "# for item in en_examples['text']:\n",
        "#   item2 = item.numpy()\n",
        "#   print(item2)\n",
        "#   print(item2.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fQJq1xB-tOn",
        "outputId": "bde9319e-e11f-4caa-eeb9-72733123732c"
      },
      "outputs": [],
      "source": [
        "# for item in en_examples['text']:\n",
        "#   print(item)\n",
        "# example_strings = [x.numpy().decode('utf-8').replace('\\n', ' ') for x in en_examples['text']]\n",
        "# print(example_strings[0], len(example_strings[0]))\n",
        "# encoded = tokenizer.encode(example_strings[0])\n",
        "# print(encoded, len(encoded))\n",
        "# for row in encoded:\n",
        "#   print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBRlikwDR2Lu"
      },
      "source": [
        "The `detokenize` method attempts to convert these token IDs back to human readable text: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tpqx1aug3W31",
        "outputId": "8ca78b7a-ac6d-4c36-abf5-a5ebaf5f15b9"
      },
      "outputs": [],
      "source": [
        "# round_trip = tokenizer.decode(encoded)\n",
        "# print(round_trip)\n",
        "# for line in round_trip.numpy():\n",
        "#   print(line.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5mGvytArL9g"
      },
      "source": [
        "The lower level `lookup` method converts from token-IDs to token text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "RpzRLzvIuN3R"
      },
      "outputs": [],
      "source": [
        "# tokens = tokenizer.lookup(encoded)\n",
        "# tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V8Ix_PNSfhV"
      },
      "source": [
        "Here you can see the \"subword\" aspect of the tokenizers. The word \"searchability\" is decomposed into \"search ##ability\" and the word \"serendipity\" into \"s ##ere ##nd ##ip ##ity\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd1NWMxjfsDd"
      },
      "source": [
        "## Setup input pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7JHK7N7tNZy"
      },
      "source": [
        "To build an input pipeline suitable for training you'll apply some transformations to the dataset.\n",
        "\n",
        "This function will be used to encode the batches of raw text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "bcRp7VcQ5m6g"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6shgzEck3FiV"
      },
      "outputs": [],
      "source": [
        "def tokenize_pairs(pt, en):\n",
        "    pt = tokenizers.pt.tokenize(pt)\n",
        "    # Convert from ragged to dense, padding with zeros.\n",
        "    pt = pt.to_tensor()\n",
        "\n",
        "    en = tokenizers.en.tokenize(en)\n",
        "    # Convert from ragged to dense, padding with zeros.\n",
        "    en = en.to_tensor()\n",
        "    return pt, en\n",
        "\n",
        "def tokenize_string(data_dict):\n",
        "\n",
        "  # for en_examples in train_examples.batch(3).take(1):\n",
        "\n",
        "  # for en in en_examples:\n",
        "  #   print(en, en_examples.get(en))\n",
        "  #   example_strings = [x.numpy().decode('utf-8').replace('\\n', ' ') for x in en_examples['text']]\n",
        "  # print(example_strings[0], len(example_strings[0]))\n",
        "  # encoded = tokenizer.encode(example_strings[0])\n",
        "  current_text = data_dict['text']\n",
        "  # clean_text = current_text.numpy().decode('utf-8').replace('\\n', ' ')\n",
        "  # encoded_text = tokenizer.encode(clean_text)\n",
        "  return current_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "BUN_jLBTwNxk"
      },
      "outputs": [],
      "source": [
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(tokenize_string, num_parallel_calls=4)) # tf.data.AUTOTUNE)\n",
        "      # .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "\n",
        "train_batches = make_batches(train_examples)\n",
        "val_batches = make_batches(val_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "vy82XMYXHMxy"
      },
      "outputs": [],
      "source": [
        "x_train = train_examples\n",
        "# y_train,\n",
        "x_test = val_examples\n",
        "# , y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBQuibYA4n0n"
      },
      "source": [
        "## Positional encoding\n",
        "\n",
        "Attention layers see their input as a set of vectors, with no sequential order. This model also doesn't contain any recurrent or convolutional layers. Because of this a \"positional encoding\" is added to give the model some information about the relative position of the tokens in the sentence. \n",
        "\n",
        "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of tokens in a sentence. So after adding the positional encoding, tokens will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n",
        "\n",
        "The formula for calculating the positional encoding is as follows:\n",
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "WhIOZjMNKujn"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "1Rz82wEs5biZ"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "1kLCla68EloE",
        "outputId": "12ce64d2-6d9e-42a1-89f1-080585067348"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 2048, 512)\n"
          ]
        },
        {
          "ename": "NotImplementedError",
          "evalue": "Cannot convert a symbolic Tensor (Reshape_1:0) to a numpy array.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_2215/1671987428.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpos_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpcolormesh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'RdBu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Depth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Position'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mpcolormesh\u001b[0;34m(alpha, norm, cmap, vmin, vmax, shading, antialiased, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m         \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshading\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshading\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialiased\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialiased\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2721\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2722\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mpcolormesh\u001b[0;34m(self, alpha, norm, cmap, vmin, vmax, shading, antialiased, *args, **kwargs)\u001b[0m\n\u001b[1;32m   6061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6062\u001b[0m         X, Y, C, shading = self._pcolorargs('pcolormesh', *args,\n\u001b[0;32m-> 6063\u001b[0;31m                                             shading=shading, kwargs=kwargs)\n\u001b[0m\u001b[1;32m   6064\u001b[0m         \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6065\u001b[0m         \u001b[0;31m# convert to one dimensional array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36m_pcolorargs\u001b[0;34m(self, funcname, shading, *args, **kwargs)\u001b[0m\n\u001b[1;32m   5518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5519\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5520\u001b[0;31m             \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5521\u001b[0m             \u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5522\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshading\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'gouraud'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nearest'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m _require_with_like = array_function_dispatch(\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0m_require_dispatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m )(require)\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     raise NotImplementedError(\"Cannot convert a symbolic Tensor ({}) to a numpy\"\n\u001b[0;32m--> 736\u001b[0;31m                               \" array.\".format(self.name))\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (Reshape_1:0) to a numpy array."
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "n, d = 2048, 512\n",
        "pos_encoding = positional_encoding(n, d)\n",
        "print(pos_encoding.shape)\n",
        "pos_encoding = pos_encoding[0]\n",
        "\n",
        "# Juggle the dimensions for the plot\n",
        "pos_encoding = tf.reshape(pos_encoding, (n, d//2, 2))\n",
        "pos_encoding = tf.transpose(pos_encoding, (2, 1, 0))\n",
        "pos_encoding = tf.reshape(pos_encoding, (d, n))\n",
        "\n",
        "plt.pcolormesh(pos_encoding, cmap='RdBu')\n",
        "plt.ylabel('Depth')\n",
        "plt.xlabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_b4ou4TYqUN"
      },
      "source": [
        "## Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s42Uydjkv0hF"
      },
      "source": [
        "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value `0` is present: it outputs a `1` at those locations, and a `0` otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "U2i8-e1s8ti9"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7BYeBCNvi7n",
        "outputId": "982dd649-2629-41c6-a02e-51c9d2738a0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor 'strided_slice_1:0' shape=(3, 1, 1, 5) dtype=float32>"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
        "create_padding_mask(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0hzukDBgVom"
      },
      "source": [
        "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
        "\n",
        "This means that to predict the third token, only the first and second token will be used. Similarly to predict the fourth token, only the first, second and the third tokens will be used and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "dVxS8OPI9uI0"
      },
      "outputs": [],
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxKGuXxaBeeE",
        "outputId": "acdf440f-f514-4a1a-f9da-10f833465938"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor 'sub:0' shape=(3, 3) dtype=float32>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = tf.random.uniform((1, 3))\n",
        "temp = create_look_ahead_mask(x.shape[1])\n",
        "temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xluDl5cXYy4y"
      },
      "source": [
        "## Scaled dot product attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsxEE_-Wa1gF"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n",
        "\n",
        "The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n",
        "\n",
        "$$\\Large{Attention(Q, K, V) = softmax_k\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V} $$\n",
        "\n",
        "The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax. \n",
        "\n",
        "For example, consider that `Q` and `K` have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of `dk`. So the *square root of `dk`* is used for scaling, so you get a consistent variance regardless of the value of `dk`. If the variance is too low the output may be too flat to optimize effectively. If the variance is too high the softmax may saturate at initialization making it difficult to learn.\n",
        "\n",
        "The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "LazzUq3bJ5SH"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead)\n",
        "  but it must be broadcastable for addition.\n",
        "\n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiqETnhCkoXh"
      },
      "source": [
        "As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n",
        "\n",
        "The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the tokens you want to focus on are kept as-is and the irrelevant tokens are flushed out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "n90YjClyInFy"
      },
      "outputs": [],
      "source": [
        "def print_out(q, k, v):\n",
        "  temp_out, temp_attn = scaled_dot_product_attention(\n",
        "      q, k, v, None)\n",
        "  print('Attention weights are:')\n",
        "  print(temp_attn)\n",
        "  print('Output is:')\n",
        "  print(temp_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAzUAf2DPlNt",
        "outputId": "0d8bab0e-b018-46bd-d9d6-863ce3687f9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "Tensor(\"Softmax:0\", shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "Tensor(\"MatMul_1:0\", shape=(1, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "temp_k = tf.constant([[10, 0, 0],\n",
        "                      [0, 10, 0],\n",
        "                      [0, 0, 10],\n",
        "                      [0, 0, 10]], dtype=tf.float32)  # (4, 3)\n",
        "\n",
        "temp_v = tf.constant([[1, 0],\n",
        "                      [10, 0],\n",
        "                      [100, 5],\n",
        "                      [1000, 6]], dtype=tf.float32)  # (4, 2)\n",
        "\n",
        "# This `query` aligns with the second `key`,\n",
        "# so the second `value` is returned.\n",
        "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zg6k-fGhgXra",
        "outputId": "f09df564-ea7b-4389-fdff-538f8f5d8854"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "Tensor(\"Softmax_1:0\", shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "Tensor(\"MatMul_3:0\", shape=(1, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# This query aligns with a repeated key (third and fourth),\n",
        "# so all associated values get averaged.\n",
        "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAq3YOzUgXhb",
        "outputId": "e7411b06-ee64-42b9-d1b2-df2172afc509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "Tensor(\"Softmax_2:0\", shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "Tensor(\"MatMul_5:0\", shape=(1, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# This query aligns equally with the first and second key,\n",
        "# so their values get averaged.\n",
        "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOz-4_XIhaTP"
      },
      "source": [
        "Pass all the queries together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dlU8Tm-hYrF",
        "outputId": "9e3e5a0d-a2eb-4a19-9984-2684aa5c0e7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "Tensor(\"Softmax_3:0\", shape=(3, 4), dtype=float32)\n",
            "Output is:\n",
            "Tensor(\"MatMul_7:0\", shape=(3, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "temp_q = tf.constant([[0, 0, 10],\n",
        "                      [0, 10, 0],\n",
        "                      [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmzGPEy64qmA"
      },
      "source": [
        "## Multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz5BMC8Kaoqo"
      },
      "source": [
        "Description from Tensorflow tutorial:  \n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
        "\n",
        "\n",
        "Multi-head attention consists of four parts:\n",
        "*    Linear layers.\n",
        "*    Scaled dot-product attention.\n",
        "*    Final linear layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPmbr6F1C-v_"
      },
      "source": [
        "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers before the multi-head attention function.\n",
        "\n",
        "In the diagram above `(K,Q,V)` are passed through sepearte linear (`Dense`) layers for each attention head. For simplicity/efficiency the code below implements this using a single dense layer with `num_heads` times as many outputs. The output is rearranged to a shape of `(batch, num_heads, ...)` before applying the attention function.\n",
        "\n",
        "The `scaled_dot_product_attention` function defined above is applied in a single call, broadcasted for efficiency. An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
        "\n",
        "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information from different representation subspaces at different positions. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "BSV3PPKsYecw"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "    # this should always be an even number, since dimension_of_model should \n",
        "    # always be dimension_of_each_head * number_of_heads_per_layer\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
        "    # returns Tensor shape: (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))  \n",
        "    # returns Tensor shape: (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  \n",
        "    # returns Tensor shape: (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D8FJue5lDyZ"
      },
      "source": [
        "Create a `MultiHeadAttention` layer to try out. At each location in the sequence, `y`, the `MultiHeadAttention` runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu94p-_-2_BX",
        "outputId": "ad8f1045-1502-4019-8508-c9833f9dc0e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorShape([Dimension(1), Dimension(60), Dimension(512)]),\n",
              " TensorShape([Dimension(1), Dimension(8), Dimension(60), Dimension(60)]))"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
        "out.shape, attn.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdDqGayx67vv"
      },
      "source": [
        "## Point wise feed forward network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBqzJXGfHK3X"
      },
      "source": [
        "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ET7xLt0yCT6Z"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, dimension_of_mlp):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dimension_of_mlp, activation='relu'),  \n",
        "      # returns Tensor shape: (batch_size, seq_len, dimension_of_mlp)\n",
        "      tf.keras.layers.Dense(d_model)  \n",
        "      # returns Tensor shape: (batch_size, seq_len, d_model)\n",
        "  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mytb1lPyOHLB",
        "outputId": "4cb3271f-a600-46e1-a13c-da339b462e5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([Dimension(64), Dimension(50), Dimension(512)])"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
        "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-YC4IOT0Qju"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHjV7ZyyD4e7"
      },
      "source": [
        "### Decoder layer\n",
        "\n",
        "  Each decoder layer consists of sublayers:\n",
        "\n",
        "  1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
        "  2.   Point wise feed forward networks (aka MLP multi-layer peceptron)\n",
        "\n",
        "  Each of these sublayers has a residual connection around it followed by \n",
        "  a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. \n",
        "  The normalization is done on the `d_model` (last) axis.\n",
        "\n",
        "  There are N decoder layers in the transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "71keOMNc0LT-"
      },
      "outputs": [],
      "source": [
        "class DecoderLayerAttnOnly(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dimension_of_mlp='unused', rate=0.1):\n",
        "    super(DecoderLayerAttnOnly, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "\n",
        "    # here is where I'll put the VAE, on the attn1\n",
        "\n",
        "    out1 = self.layernorm1(attn1 + x) # I think x is the residual stream here\n",
        "\n",
        "    return out1, attn_weights_block1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "-WF_oaXl1aSk"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, dimension_of_mlp, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dimension_of_mlp) # aka MLP, multi-layer perceptron\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  \n",
        "    # attn1.shape: (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "\n",
        "    # here is where I'll put the compression (e.g. infoVAE), on the attn1\n",
        "\n",
        "    out1 = self.layernorm1(attn1 + x) # I think x is the residual stream here\n",
        "\n",
        "    ffn_output = self.ffn(out1)  \n",
        "    # returns Tensor shape: (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "\n",
        "    # here is where I could put a second VAE on the ffn_output\n",
        "    \n",
        "    out2 = self.layernorm2(ffn_output + out1)  \n",
        "    # returns Tensor shape: (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out2, attn_weights_block1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "fcAcCSSn0Ldl"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, num_layers, d_model, num_heads, dimension_of_mlp, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1, attention_only=False):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = ([DecoderLayerAttnOnly(d_model, num_heads, dimension_of_mlp, rate)\n",
        "                       for _ in range(num_layers)] if attention_only else \n",
        "                       [DecoderLayer(d_model, num_heads, dimension_of_mlp, rate)\n",
        "                       for _ in range(num_layers)])\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  \n",
        "    # returns Tensor shape: (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1 = self.dec_layers[i](x, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}'] = block1\n",
        "\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBlIK3-p0WpG",
        "outputId": "22248042-0114-4058-ca52-eaccf3a29bb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample_decoder_layer_output.shape (64, 50, 512)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(TensorShape([Dimension(64), Dimension(26), Dimension(512)]),\n",
              " TensorShape([Dimension(64), Dimension(8), Dimension(26), Dimension(26)]))"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test sample initialization of the decoder layer\n",
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_decoder_layer_output, _ = sample_decoder_layer(\n",
        "    tf.random.uniform((64, 50, 512)),\n",
        "    False, None, None)\n",
        "\n",
        "print('sample_decoder_layer_output.shape',sample_decoder_layer_output.shape)   \n",
        "# returns Tensor shape: (batch_size, target_seq_len, d_model)\n",
        "\n",
        "# and the full Decoder wrapper...\n",
        "\n",
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8,\n",
        "                         dimension_of_mlp=2048, target_vocab_size=8000,\n",
        "                         maximum_position_encoding=5000)\n",
        "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "output, attn = sample_decoder(temp_input,\n",
        "                              training=False,\n",
        "                              look_ahead_mask=None,\n",
        "                              padding_mask=None)\n",
        "\n",
        "output.shape, attn['decoder_layer2'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y54xnJnuYgJ7"
      },
      "source": [
        "## Create the Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uERO1y54cOKq"
      },
      "source": [
        "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "PED3bIpOYkBu"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, decoder_num_layers, d_model, num_heads, dimension_of_mlp, \n",
        "               vocab_size,\n",
        "               max_position_encoding, rate=0.1, attention_only=False):\n",
        "    super().__init__()\n",
        "    self.decoder = Decoder(decoder_num_layers, d_model, num_heads, dimension_of_mlp,\n",
        "                             vocab_size, max_position_encoding, rate=rate, \n",
        "                           attention_only=attention_only)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
        "    self.max_position_encoding = max_position_encoding\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    # Keras models prefer if you pass all your inputs in the first argument\n",
        "    padding_mask, look_ahead_mask = self.create_masks(inputs)\n",
        "\n",
        "    output, attention_weights = self.decoder(inputs, training, look_ahead_mask, padding_mask)  \n",
        "    # returns Tensor shape: (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "    final_output = self.final_layer(output)  \n",
        "    # returns Tensor shape:(batch_size, inp_seq_len, vocab_size)\n",
        "\n",
        "    return final_output, attention_weights\n",
        "\n",
        "  def create_masks(self, inputs):\n",
        "    padding_mask = create_padding_mask(inputs)\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(inputs)[1])\n",
        "    look_ahead_mask = tf.maximum(padding_mask, look_ahead_mask)\n",
        "    return padding_mask, look_ahead_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ4fbQcIkHW1",
        "outputId": "de5da870-5705-468f-ea1b-f4d9793bb7d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([Dimension(64), Dimension(20), Dimension(8000)])"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp_max_position_encoding = 20\n",
        "sample_transformer = Transformer(\n",
        "    decoder_num_layers=2, d_model=512, num_heads=8, dimension_of_mlp=2048,\n",
        "    vocab_size=8000,\n",
        "    max_position_encoding=temp_max_position_encoding, attention_only=True)\n",
        "\n",
        "temp_batch_size = 64\n",
        "temp_input = tf.random.uniform((temp_batch_size, temp_max_position_encoding), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = sample_transformer(temp_input, training=False)\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsINyf1VEQLC"
      },
      "source": [
        "## Set hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euddz-VrMArg"
      },
      "source": [
        "The details from Tranformer Circuit team's initial paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvyrVUV1L7sD"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnJn5SLA2ahP",
        "outputId": "cf75cf9c-bf56-4320-d836-485cba3fe6e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'attention_only': True,\n",
            " 'batch_size': 64,\n",
            " 'd_model': 768,\n",
            " 'decoder_num_layers': 2,\n",
            " 'dimension_of_mlp': 3072,\n",
            " 'dimensions_per_head': 64,\n",
            " 'dropout_rate': 0.1,\n",
            " 'epochs': 2,\n",
            " 'metrics': ['loss', 'accuracy', 'val_loss', 'val_accuracy'],\n",
            " 'model_context_tokens': 20,\n",
            " 'model_name': 'Decoder_2layer_12head_attnOnly_v1',\n",
            " 'num_heads_per_layer': 12,\n",
            " 'vocab_size': 50257}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pprint import pprint\n",
        "hyperparameters = {\n",
        "    'batch_size':64,\n",
        "    'decoder_num_layers':2,\n",
        "    'num_heads_per_layer':12, \n",
        "    'dimensions_per_head': 64,\n",
        "    'dropout_rate':0.1,\n",
        "    'model_context_tokens': 20, # using also as max_positional_encoding for this purpose\n",
        "    # context tokens set to much smaller than TC's 2048 for now for training speed\n",
        "    'attention_only':True,\n",
        "    'epochs':2,\n",
        "    'vocab_size':tokenizer.vocab_size,\n",
        "    'metrics':['loss', 'accuracy', 'val_loss', 'val_accuracy']\n",
        "}\n",
        "# dimension_of_model (d_model) originally set in example code to 128\n",
        "hyperparameters['d_model'] = (hyperparameters['num_heads_per_layer'] * \n",
        "                              hyperparameters['dimensions_per_head']\n",
        ")\n",
        "# dimension_of_mlp originally set in example code to 4 * d_model = 512\n",
        "hyperparameters['dimension_of_mlp'] = hyperparameters['d_model'] * 4\n",
        "hyperparameters['model_name'] = f\"Decoder_\\\n",
        "{hyperparameters['decoder_num_layers']}layer_\\\n",
        "{hyperparameters['num_heads_per_layer']}head_\\\n",
        "{'attnOnly' if hyperparameters['attention_only'] else 'mlp'}_\\\n",
        "v1\"\n",
        "pprint(hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQT7xNQQxA_5",
        "outputId": "dcb8f375-f3ea-43f5-96cf-18c0d6416cdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<zip object at 0x7f23980402d0>\n"
          ]
        }
      ],
      "source": [
        "# planing tests to run on varying compression\n",
        "# compression_ratio = vae_latent_size / d_model \n",
        "vae_latent_space_sizes = [1,5,10,20,50,100,200]\n",
        "compression_ratios = [x / hyperparameters['d_model'] for x in vae_latent_space_sizes]\n",
        "print(zip(vae_latent_space_sizes, compression_ratios))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "ari5Slk-jsCB"
      },
      "outputs": [],
      "source": [
        "special_tokens = ['_START_ARTICLE_', '_START_SECTION_', '_START_PARAGRAPH_', '_NEWLINE_']\n",
        "paragraph_token = '_START_PARAGRAPH_'\n",
        "newline_token = '_NEWLINE_'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "HuZs7Mghxip1"
      },
      "outputs": [],
      "source": [
        "def convert_wikidata_to_tokens(data):\n",
        "  encoded_tensors = []\n",
        "  for inp in data:\n",
        "    split_text = inp.numpy().decode('utf-8').replace('\\n', ' ').replace(newline_token, '\\n').split(' ')\n",
        "    paragraph_start = split_text.index(paragraph_token) + 1 if paragraph_token in split_text else None\n",
        "    if paragraph_start is None:\n",
        "      continue\n",
        "    # only encode as many words as we need context tokens\n",
        "    raw_text = ' '.join(split_text[paragraph_start: hyperparameters['model_context_tokens'] + paragraph_start + 1])\n",
        "\n",
        "    # need to trim encoded_text since some things like punctuation become tokens\n",
        "    encoded_text = tokenizer.encode(raw_text)[:hyperparameters['model_context_tokens']]\n",
        "\n",
        "    encoded_tensor = tf.constant(encoded_text)\n",
        "\n",
        "    # if there are tensors shorter than the desired length, need to left pad them\n",
        "    if encoded_tensor.shape[0] < hyperparameters['model_context_tokens']:\n",
        "      padding = tf.constant([[hyperparameters['model_context_tokens'] - encoded_tensor.shape[0], 0]])\n",
        "      encoded_tensor = tf.pad(encoded_tensor, padding)\n",
        "\n",
        "    encoded_tensors.append(encoded_tensor)\n",
        "  \n",
        "  return tf.stack(encoded_tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "rNJvAMo5_pC-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<DatasetV1Adapter shapes: {text: (?,), version_id: (?,), wikidata_id: (?,)}, types: {text: tf.string, version_id: tf.string, wikidata_id: tf.string}> <class 'tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter'>\n",
            "{'_dataset': <TakeDataset shapes: {text: (?,), version_id: (?,), wikidata_id: (?,)}, types: {text: tf.string, version_id: tf.string, wikidata_id: tf.string}>, '_variant_tensor_attr': <tf.Tensor 'TakeDataset_8:0' shape=() dtype=variant>, '_self_setattr_tracking': True, '_self_unconditional_checkpoint_dependencies': [TrackableReference(name='_variant_tracker', ref=<tensorflow.python.data.ops.dataset_ops._VariantTracker object at 0x7f238be44a90>)], '_self_unconditional_dependency_names': {'_variant_tracker': <tensorflow.python.data.ops.dataset_ops._VariantTracker object at 0x7f238be44a90>}, '_self_unconditional_deferred_dependencies': {}, '_self_update_uid': -1, '_self_name_based_restores': set(), '_variant_tracker': <tensorflow.python.data.ops.dataset_ops._VariantTracker object at 0x7f238be44a90>, '_graph_attr': <tensorflow.python.framework.ops.Graph object at 0x7f24000e87d0>}\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "__iter__() is only supported inside of tf.function or when eager execution is enabled.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_2215/271260214.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mraw_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2034\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    341\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIteratorV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n\u001b[0m\u001b[1;32m    344\u001b[0m                          \"or when eager execution is enabled.\")\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: __iter__() is only supported inside of tf.function or when eager execution is enabled."
          ]
        }
      ],
      "source": [
        "raw_val = val_examples.batch(3).take(1)\n",
        "\n",
        "print(raw_val, type(raw_val)) \n",
        "print(raw_val.__dict__)\n",
        "for item in val_examples.batch(10000).take(1):\n",
        "    raw_val = item\n",
        "    break\n",
        "val_sample = convert_wikidata_to_tokens(raw_val.get('text'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYEGhEOtzn5W"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOmWW--yP3zx"
      },
      "source": [
        "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "$$\\Large{lrate = d_{model}^{-0.5} * \\min(step{\\_}num^{-0.5}, step{\\_}num \\cdot warmup{\\_}steps^{-1.5})}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "iYQdOO1axwEI"
      },
      "outputs": [],
      "source": [
        "# Nathan says: I don't yet understand why it is necessary for the learning rate schedule\n",
        "# to be proportional to the dimension of the model. I should do more background reading.\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "7r4scdulztRx"
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(hyperparameters['d_model'])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "f33ZCgvHpPdG",
        "outputId": "07a83a0b-1aca-4a85-cf57-e702899ca1d3"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Tensor' object has no attribute 'ndim'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_2215/3045249730.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtemp_learning_rate_schedule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomSchedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_learning_rate_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Learning Rate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2757\u001b[0m     return gca().plot(\n\u001b[1;32m   2758\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2759\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1630\u001b[0m         \"\"\"\n\u001b[1;32m   1631\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1632\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1633\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1650\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;31m# NumPy 1.19 will warn on ragged input, and we can't actually use it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     message='Support for multi-dimensional indexing')\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m                 \u001b[0;31m# we have definitely hit a pandas index or series object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;31m# cast to a numpy array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'ndim'"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "temp_learning_rate_schedule = CustomSchedule(hyperparameters['d_model'])\n",
        "\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgkDE7hzo8r5"
      },
      "source": [
        "## Loss and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxGJtoDuYIHL"
      },
      "source": [
        "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "MlhsJMm0TW_B"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "67oqVHiT0Eiu"
      },
      "outputs": [],
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  # print(real, pred)\n",
        "  argmax_pred = tf.argmax(pred, axis=2, output_type=tf.dtypes.int32)\n",
        "  # print(argmax_pred)\n",
        "  accuracies = tf.equal(real, argmax_pred)\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  # print(mask)\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "phlyxMnm-Tpx"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "validation_loss = tf.keras.metrics.Mean(name='validation_loss')\n",
        "validation_accuracy = tf.keras.metrics.Mean(name='validation_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeHumfr7zmMa"
      },
      "source": [
        "## Training and checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ_fTVDt-59t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "UiysUa--4tOU"
      },
      "outputs": [],
      "source": [
        "\n",
        "transformer = Transformer(\n",
        "    decoder_num_layers=hyperparameters['decoder_num_layers'],\n",
        "    d_model=hyperparameters['d_model'],\n",
        "    num_heads=hyperparameters['num_heads_per_layer'],\n",
        "    dimension_of_mlp=hyperparameters['dimension_of_mlp'],\n",
        "    vocab_size=hyperparameters['vocab_size'],\n",
        "    max_position_encoding=hyperparameters['model_context_tokens'],\n",
        "    rate=hyperparameters['dropout_rate'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "oGnkIIDwK9lg"
      },
      "outputs": [],
      "source": [
        "# Create the checkpoint path and the checkpoint manager. \n",
        "# This will be used to save checkpoints every `n` epochs.\n",
        "\n",
        "def save_metadata_to_json(checkpoint_path, hyperparameters, history, epoch, batch):\n",
        "  custom_metadata = {'history':history, 'hyperparameters':hyperparameters, 'epoch':epoch, 'batch':batch}\n",
        "  with open(f\"{checkpoint_path}/metadata.json\", 'w') as f:\n",
        "    json.dump(custom_metadata, f)\n",
        "\n",
        "def load_metadata_from_json(checkpoint_path, hyperparameters):\n",
        "  custom_metadata = {}\n",
        "  with open(f\"{checkpoint_path}/metadata.json\", 'r') as f:\n",
        "    custom_metadata = json.load(f)\n",
        "  return custom_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "hXrDhfQNerdX"
      },
      "outputs": [],
      "source": [
        "# Uncomment this for resetting checkpoint manager for debugging purposes\n",
        "# ckpt_manager = None\n",
        "# ckpt = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNhuYfllndLZ",
        "outputId": "6a938852-9f7e-4d00-ac78-a68ad3191b2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No checkpoint found. Starting fresh\n"
          ]
        }
      ],
      "source": [
        "checkpoint_path = f\"{project_path}/models/{hyperparameters['model_name']}/train\"\n",
        "Path(checkpoint_path).mkdir(parents=True, exist_ok=True)\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "custom_metadata = {}\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  custom_metadata = load_metadata_from_json(checkpoint_path, hyperparameters)\n",
        "  loaded_hyperparameters = custom_metadata['hyperparameters']\n",
        "  print('loaded hyperparameters:')\n",
        "  pprint(loaded_hyperparameters)\n",
        "  for item in loaded_hyperparameters:\n",
        "    if hyperparameters.get(item) != loaded_hyperparameters[item]:\n",
        "      print(f'''mismatch of {item} between hyperparameters: \n",
        "      current {hyperparameters.get(item)}, loaded {loaded_hyperparameters[item]}''')\n",
        "  for item in hyperparameters:\n",
        "    if hyperparameters[item] != loaded_hyperparameters.get(item):\n",
        "      print(f'''mismatch of {item} between hyperparameters: \n",
        "      current {hyperparameters.get(item)}, loaded {loaded_hyperparameters.get(item)}''')\n",
        "  metrics = hyperparameters['metrics']\n",
        "  history = custom_metadata['history']\n",
        "  print(f'Latest checkpoint restored!! {ckpt_manager.latest_checkpoint}')\n",
        "else:\n",
        "  metrics = hyperparameters['metrics']\n",
        "  history = {}\n",
        "  for m in metrics:\n",
        "    history[m] = []\n",
        "  print('No checkpoint found. Starting fresh')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Di_Yaa1gf9r"
      },
      "source": [
        "The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. `tar_real` is that same input shifted by 1: At each location in `tar_input`, `tar_real` contains the  next token that should be predicted.\n",
        "\n",
        "For example, `sentence` = \"SOS A lion in the jungle is sleeping EOS\"\n",
        "\n",
        "`tar_inp` =  \"SOS A lion in the jungle is sleeping\"\n",
        "\n",
        "`tar_real` = \"A lion in the jungle is sleeping EOS\"\n",
        "\n",
        "The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next. \n",
        "\n",
        "During training this example uses teacher-forcing (like in the [text generation tutorial](https://www.tensorflow.org/text/tutorials/text_generation)). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
        "\n",
        "As the transformer predicts each token, *self-attention* allows it to look at the previous tokens in the input sequence to better predict the next token.\n",
        "\n",
        "To prevent the model from peeking at the expected output the model uses a look-ahead mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE_JfC27hkZl"
      },
      "outputs": [],
      "source": [
        "  # # Uncomment to reset the history for debugging\n",
        "  # history = {}\n",
        "  # for m in metrics:\n",
        "  #   history[m] = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy0H-t_9L6-y",
        "outputId": "ff47df64-c6a4-48e5-a478-20df5cd2b1ac"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'val_sample' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_2215/1500136531.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'val_sample' is not defined"
          ]
        }
      ],
      "source": [
        "print(val_sample[0:0 + hyperparameters['batch_size'], 0:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "iJwmp9OE29oj"
      },
      "outputs": [],
      "source": [
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int32)\n",
        "]\n",
        "\n",
        "\n",
        "# @tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp):\n",
        "  # print(type(inp), inp.shape)\n",
        "  input = inp[:, 0:-1]\n",
        "  # print(type(inp), inp.shape)\n",
        "  target = inp[:, 1:]\n",
        "\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(input,\n",
        "                                 training = True)\n",
        "    loss = loss_function(target, predictions)\n",
        "    \n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(target, predictions))\n",
        "  \n",
        "def val_step(val_inputs):\n",
        "  val_input = val_inputs[:, 0:-1]\n",
        "  val_target = val_inputs[:, 1:]\n",
        "  val_predictions, _ = transformer(val_input, training = False)\n",
        "  val_loss = loss_function(val_target, val_predictions)\n",
        "  validation_loss(val_loss)\n",
        "  validation_accuracy(accuracy_function(val_target, val_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "bbvmaKNiznHZ",
        "outputId": "0d621f98-0704-46a7-c9f3-00b687d3d4b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyperparameters:\n",
            "{'attention_only': True,\n",
            " 'batch_size': 64,\n",
            " 'd_model': 768,\n",
            " 'decoder_num_layers': 2,\n",
            " 'dimension_of_mlp': 3072,\n",
            " 'dimensions_per_head': 64,\n",
            " 'dropout_rate': 0.1,\n",
            " 'epochs': 2,\n",
            " 'metrics': ['loss', 'accuracy', 'val_loss', 'val_accuracy'],\n",
            " 'model_context_tokens': 20,\n",
            " 'model_name': 'Decoder_2layer_12head_attnOnly_v1',\n",
            " 'num_heads_per_layer': 12,\n",
            " 'vocab_size': 50257}\n",
            "training epochs from 1 to 2\n",
            "current_batch_num: 1\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "__iter__() is only supported inside of tf.function or when eager execution is enabled.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_2215/1863822330.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mbatch_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_batch_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mcurrent_batch_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mstacked_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_wikidata_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2034\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/venv37/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    341\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIteratorV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n\u001b[0m\u001b[1;32m    344\u001b[0m                          \"or when eager execution is enabled.\")\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: __iter__() is only supported inside of tf.function or when eager execution is enabled."
          ]
        }
      ],
      "source": [
        "current_epoch = custom_metadata.get('epoch', 1)\n",
        "current_batch_num = custom_metadata.get('batch', 1)\n",
        "print('Hyperparameters:')\n",
        "pprint(hyperparameters)\n",
        "print(f\"training epochs from {current_epoch} to {hyperparameters['epochs']}\")\n",
        "print('current_batch_num:', current_batch_num)\n",
        "for epoch in range(current_epoch, hyperparameters['epochs'] + 1):\n",
        "  start = time.time()\n",
        "  custom_metadata['epoch'] = epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  validation_loss.reset_states()\n",
        "  validation_accuracy.reset_states()\n",
        "\n",
        "  batch_num = current_batch_num\n",
        "  current_batch_num = 1\n",
        "  for inputs in train_batches:\n",
        "    stacked_tensors = convert_wikidata_to_tokens(inputs)\n",
        "\n",
        "    train_step(stacked_tensors)\n",
        "    val_count = 0\n",
        "    for i in range(0, 10000 - hyperparameters['batch_size'], hyperparameters['batch_size']):\n",
        "      if randint(0, 18) < 18 and (val_count > 8 or i < 5000):\n",
        "        continue\n",
        "      val_step(val_sample[i:i + hyperparameters['batch_size'], :])\n",
        "      val_count += 1\n",
        "\n",
        "    batch_num += 1\n",
        "    custom_metadata['batch'] = batch_num\n",
        "    current_metrics = {'loss': train_loss.result(), \n",
        "                       'accuracy': train_accuracy.result(),\n",
        "                       'val_loss': validation_loss.result(),\n",
        "                       'val_accuracy': validation_accuracy.result(),\n",
        "                       'val_samples': val_count}\n",
        "    for metric in metrics:\n",
        "      history[metric] += [float(current_metrics[metric])]\n",
        "   \n",
        "    if batch_num % 20 == 0:\n",
        "      clear_output(wait=True)\n",
        "      f, axs = plt.subplots(1, len(['loss', 'accuracy']), figsize=(15,5))\n",
        "      print(f'Epoch {epoch} Batch {batch_num} Loss {train_loss.result():.4f} \\\n",
        "      Accuracy {train_accuracy.result():.4f} Val samples {val_count}', flush=True)\n",
        "      for i, metric in enumerate(['loss', 'accuracy']): \n",
        "        axs[i].plot(range(len(history[metric])), \n",
        "                    history[metric], \n",
        "                    label='train_' + metric)\n",
        "        if history.get('val_' + metric):\n",
        "            axs[i].plot(range(len(history[metric])), \n",
        "                        history['val_' + metric], \n",
        "                        label='val_' + metric)\n",
        "            \n",
        "        axs[i].legend()\n",
        "        axs[i].grid()\n",
        "        axs[i].set_title(metric)\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "      \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    validation_loss.reset_states()\n",
        "    validation_accuracy.reset_states()\n",
        "      \n",
        "    if batch_num % 100 == 0:\n",
        "      ckpt_save_path = ckpt_manager.save()\n",
        "      save_metadata_to_json(checkpoint_path, hyperparameters, history, epoch, batch_num)\n",
        "      print(f'Saving checkpoint for epoch {epoch} batch {batch_num} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} \\\n",
        "  Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {(time.time() - start) / 60.0:.2f} minutes\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnTJzYZ4fxj0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "transformer_decoder_plus_vae.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0599716d3c0e435f9dc3130b42b14bea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49e6c8996c01473586127284db1f5969",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc635c94f187432195a8c6bbf4908c02",
            "value": 665
          }
        },
        "061ef147b481472fb09d67a310da5839": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "168d699b4f0047f1b2cf91506a734c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a16dc0b40e574e0596325474dc361c03",
              "IPY_MODEL_1ab1e05b56834c1fa12b09ac2e5bb826",
              "IPY_MODEL_3576090d28654cf2ba0ccb9ad885b72c"
            ],
            "layout": "IPY_MODEL_f4b63b655b4a44d3b04d36e9d66239bb"
          }
        },
        "17bac3e1c9d74664a57097206907f829": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ab1e05b56834c1fa12b09ac2e5bb826": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ff7c26f261a4842a355dab24f83c17c",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac736c3420f243059a435030c8489458",
            "value": 456318
          }
        },
        "283c81df98ad474aac287455d2c0fc5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7639d95150314e78a5ea2ef00350f4e7",
              "IPY_MODEL_0599716d3c0e435f9dc3130b42b14bea",
              "IPY_MODEL_752babd396f14c0495666352d67ceb4c"
            ],
            "layout": "IPY_MODEL_5478a53244b64995bc8d0d20bd97f65f"
          }
        },
        "2a5eae85e5eb43d7abe9773a72015dad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5bb3969275d451d8add76cbb935eeff",
            "placeholder": "​",
            "style": "IPY_MODEL_43750abaf77c4954acbd000a8c00183c",
            "value": "Downloading: 100%"
          }
        },
        "3576090d28654cf2ba0ccb9ad885b72c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7ea46cb41ea4ab0aa6a4eed19f60c6f",
            "placeholder": "​",
            "style": "IPY_MODEL_584f2f021a7e4a4b9d20381b8963d6c1",
            "value": " 446k/446k [00:00&lt;00:00, 8.16kB/s]"
          }
        },
        "43750abaf77c4954acbd000a8c00183c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "499923c8538c4508ac0bc51efeefe85d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49e6c8996c01473586127284db1f5969": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f2b0c7d78c1421d9be36528cc97c8eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "541b3afe67ab41838f51fbcac7f9f379": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5478a53244b64995bc8d0d20bd97f65f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "559d6d68e25f4a67b6397f23e85efd07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_541b3afe67ab41838f51fbcac7f9f379",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75530da518a941e88e38e1581547f75b",
            "value": 1042301
          }
        },
        "584f2f021a7e4a4b9d20381b8963d6c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ff7c26f261a4842a355dab24f83c17c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "752babd396f14c0495666352d67ceb4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d32be5bb06cc480c85c2cd78805d91c3",
            "placeholder": "​",
            "style": "IPY_MODEL_061ef147b481472fb09d67a310da5839",
            "value": " 665/665 [00:00&lt;00:00, 8.50kB/s]"
          }
        },
        "75530da518a941e88e38e1581547f75b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7639d95150314e78a5ea2ef00350f4e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_499923c8538c4508ac0bc51efeefe85d",
            "placeholder": "​",
            "style": "IPY_MODEL_4f2b0c7d78c1421d9be36528cc97c8eb",
            "value": "Downloading: 100%"
          }
        },
        "7f006303fc2c419fb09cf892b181a08c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a5eae85e5eb43d7abe9773a72015dad",
              "IPY_MODEL_559d6d68e25f4a67b6397f23e85efd07",
              "IPY_MODEL_d192779a902048528a22d8f8fcb0d54f"
            ],
            "layout": "IPY_MODEL_17bac3e1c9d74664a57097206907f829"
          }
        },
        "a16dc0b40e574e0596325474dc361c03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9a68829fa634310895b25f25c3c2c32",
            "placeholder": "​",
            "style": "IPY_MODEL_b7f2098fd20847db89f41b2b007e7e06",
            "value": "Downloading: 100%"
          }
        },
        "a7ea46cb41ea4ab0aa6a4eed19f60c6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9a68829fa634310895b25f25c3c2c32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac736c3420f243059a435030c8489458": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7f2098fd20847db89f41b2b007e7e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5bb3969275d451d8add76cbb935eeff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d192779a902048528a22d8f8fcb0d54f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4b96ef6a6f64ea8af72f4813a0e7ce5",
            "placeholder": "​",
            "style": "IPY_MODEL_dc66da1970b949c2b2bec1f3a87405c3",
            "value": " 0.99M/0.99M [00:00&lt;00:00, 6.43kB/s]"
          }
        },
        "d32be5bb06cc480c85c2cd78805d91c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc635c94f187432195a8c6bbf4908c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc66da1970b949c2b2bec1f3a87405c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4b96ef6a6f64ea8af72f4813a0e7ce5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4b63b655b4a44d3b04d36e9d66239bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
